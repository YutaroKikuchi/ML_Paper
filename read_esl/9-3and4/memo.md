# 9.3 抑制的規則導出法

patient rule induction method: PRIM

CART(木に基づく手法)では、特徴空間を箱型に分割し、応答変数の平均値ができるだけ異なるように、値を割り当てる手法

一方PRIMは特徴空間を箱型に分割するが、箱の中で応答変数の平均がもっとも高くなる箱を選択する。

一連の流れ

1. 訓練データ全てを含む最大の箱から始める。

2. 応答変数の平均値が高くなるデータ点を分離するために、箱を徐々に縮める。縮め方は予測変数Xの値がもっとも高いか低いデータ点を一定の割合で分離するというやり方。

3. ステップ2を箱内に残った観測値の数がある最小値になるまで繰り返す。

4. ステップ2とは逆の過程で箱の中での平均が増加する限り、箱を拡張する。

5. ステップ1 ~ 4より、入る観測値の数が異なる一連の箱が得られる。黄砂確認を用いて、この中から箱を選ぶ。

6. 選んだ箱内のデータ点を取り除き、ステップ2~5を繰り返して次の箱を得る。

再帰的に領域を分割するというよりも、応答変数が取りうる範囲を縮めていくというイメージかな。

↓ステップ2の動作例


青い点と赤い点を分解するための箱を作る様子。箱に残った点の平均が最大になるように、一定の割合(この例では0.1)で点を隔離するという操作を繰り返している。この例では27回の反復で終了した。

応答変数の平均が最大になるような箱を作るので、回帰問題を対象とした手法。 分類問題に適用するなら、2クラスの出力で考えるしかない。

PRIMのCARTより有利な点は、抑制的なことである。「抑制的」の具体的説明はなかったが、おそらく分割数が多くなる→分割パターンが良い解を見つけやすいということなのだろう。具体的には、CARTの分割回数は$\log_2(N) - 1$回 に対してPRIMの分割回数は$-\log(N)/\log(1-\alpha)$回となる。N=128ならCARTが6でPRIMはおよそ46にもなる。

# 9.4 多変量適応的回帰スプライン

multivariate adaptive regression spline : MARS

**MARSで使用する基底関数**


各データ点$x_{ij}$のところに接点がある基底関数のペアを考えることが基本的アイデア。基底関数の数は最大$2N_p$個となる。

基底関数の集合$C$は次のように表現できる。
$$
C = \{(X_j -t)_+, (t-X_j)_+\}
t \in \{x_{1j}, \cdots x_{Nj}\}, j=1\cdots p
$$

モデルは次のように表せる。

$$
f(x) = \beta_0 + \sum^M_{m=1}\beta_mh_m(X)
$$
$h_m(X)$は集合$C$に含まれる関数あるいは2個以上の積である。$h$が決まったら、線形回帰のように$\beta$を決定すれば良い。

**モデルの構築方法**
↓が図解


左が現在のモデルに含まれる基底関数で定数$h(X)=1$だけである。右はモデルを構築する際に考慮する全ての基底関数の候補。
各段階でモデル内の基底関数と候補の基底関数との全ての積を考え、残差がもっとも減少する積が現在のモデルに加えられる。

この図は3ステップだけを示しており、選ばれた関数が赤で示されている。

上記の過程が終わると、モデル$f(X)$が完成する。このモデルは過学習しているため、項を削除する手続きをとる。
対象となる項をモデルから削除した時の残差2乗誤差の増加が最小になる項を削っていき、項の数$\lambda$について最適なモデル$\hat{f}_\lambda$が推定される。


最適な$\lambda$は交差検証を使ってもいいが、ここは計算量削減のため、一般交差検証(Generalized Cross Validation)を用いる。

**なぜこの基底関数なのか**
なぜ$(X_j -t)_+, (t-X_j)_+$という線形区分的な基底関数を用いるのか。
→ それは局所的に働くということが鍵となる。

基底関数同士が下の図のように積になると、両方の関数が0でない小さな部分だけが非ゼロになる。結果として、回帰面は本当に必要な部分だけ、非ゼロの要素を局所的に使って少しずつ構成される。
多項式の基底関数では非ゼロの領域があちこちにできてしまうので難しい。

また、計算量的にもこの基底関数の方が都合がよく、$O(N^2)$のところを$O(N)$になる。
(詳しくは本書を参照)


## 9.4.1 例: スパムメール (続き)
以前にも示したスパムメールにも適用する。結果として誤分類率は5.5%では足りで下げ止まり、前に述べた一般化加法的モデルの値(5.3%)よりわずかに大きい。

## 9.4.2 例: 試行データ
3種類のシナリオを作ってMARSを比較。結果として高次の交差項を含むシナリオ3で大きく性能を落としていた。

## 9.4.3 その他の話題


