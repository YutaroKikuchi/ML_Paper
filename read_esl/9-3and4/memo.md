# 9.3 抑制的規則導出法

patient rule induction method: PRIM

CART(木に基づく手法)では、特徴空間を箱型に分割し、応答変数の平均値ができるだけ異なるように、値を割り当てる手法

一方PRIMは特徴空間を箱型に分割するが、箱の中で応答変数の平均がもっとも高くなる箱を選択する。

一連の流れ

1. 訓練データ全てを含む最大の箱から始める。

2. 応答変数の平均値が高くなるデータ点を分離するために、箱を徐々に縮める。縮め方は予測変数Xの値がもっとも高いか低いデータ点を一定の割合で分離するというやり方。

3. ステップ2を箱内に残った観測値の数がある最小値になるまで繰り返す。

4. ステップ2とは逆の過程で箱の中での平均が増加する限り、箱を拡張する。

5. ステップ1 ~ 4より、入る観測値の数が異なる一連の箱が得られる。交差確認を用いて、この中から箱を選ぶ。

6. 選んだ箱内のデータ点を取り除き、ステップ2~5を繰り返して次の箱を得る。

再帰的に領域を分割するというよりも、応答変数が取りうる範囲を縮めていくというイメージかな。

↓ステップ2の動作例

![hoge](https://github.com/talesofyousan/ML_Paper/blob/master/read_esl/9-3and4/prim_algorithm.png?raw=true)

青い点と赤い点を分解するための箱を作る様子。箱に残った点の平均が最大になるように、一定の割合(この例では0.1)で点を隔離するという操作を繰り返している。この例では27回の反復で終了した。

応答変数の平均が最大になるような箱を作るので、回帰問題を対象とした手法。 分類問題に適用するなら、2クラスの出力で考えるしかない。

PRIMのCARTより有利な点は、抑制的なことである。「抑制的」の具体的説明はなかったが、おそらく分割数が多くなる→分割パターンが良い解を見つけやすいということなのだろう。具体的には、CARTの分割回数は$\log_2(N) - 1$回 に対してPRIMの分割回数は$-\log(N)/\log(1-\alpha)$回となる。N=128ならCARTが6でPRIMはおよそ46にもなる。

# 9.4 多変量適応的回帰スプライン

multivariate adaptive regression spline : MARS

回帰のための適応的な手続きであり漸次的な線形回帰の一般化、あるいはCART法の変種で、回帰の場合に性能を改善するものとみなすこともできる。
各データ点$x_{ij}$のところに節点がある基底関数のペアを考え、線形回帰のように組み合わせて行くのが基本的アイデア。

**MARSで使用する基底関数**

![hoge](https://github.com/talesofyousan/ML_Paper/blob/master/read_esl/9-3and4/kitei_kansuu.png?raw=true)

![hoge](https://github.com/talesofyousan/ML_Paper/blob/master/read_esl/9-3and4/kitei_kansu_graph.png?raw=true)

$t$は節点と呼ばれ、データ点ごとに定義できるため、訓練データの値がそれぞれ異なっていたら、基底関数の数は$2N$個となる。
基底関数の集合$C$は次のように表現できる。
$$
C = \{(X_j -t)_+, (t-X_j)_+\}
t \in \{x_{1j}, \cdots x_{Nj}\}, j=1\cdots p
$$

モデルは次のように表せる。

$$
f(x) = \beta_0 + \sum^M_{m=1}\beta_mh_m(X)
$$
$h_m(X)$は集合$C$に含まれる関数あるいは2個以上の積である。$h$が決まったら、線形回帰のように$\beta$を決定すれば良い。

**モデルの構築方法**

↓が図解

![hoge](https://github.com/talesofyousan/ML_Paper/blob/master/read_esl/9-3and4/model_build_process.png?raw=true)

左が現在のモデルに含まれる基底関数で定数$h(X)=1$だけである。右はモデルを構築する際に考慮する全ての基底関数の候補。
各段階でモデル内の基底関数と候補の基底関数との全ての積を考え、残差がもっとも減少する積が現在のモデルに加えられる。

この図は3ステップだけを示しており、選ばれた関数が赤で示されている。

上記の過程が終わると、モデル$f(X)$が完成する。このモデルは過学習しているため、項を削除する手続きをとる。
対象となる項をモデルから削除した時の残差2乗誤差の増加が最小になる項を削っていき、項の数$\lambda$について最適なモデル$\hat{f}_\lambda$が推定される。


最適な$\lambda$は交差検証を使ってもいいが、ここは計算量削減のため、一般交差検証(Generalized Cross Validation)を用いる。

**なぜこの線形区分的な基底関数なのか**

なぜ$(X_j -t)_+, (t-X_j)_+$という線形区分的な基底関数を用いるのか。
→ それは局所的に働くということが鍵となる。

![hoge](https://github.com/talesofyousan/ML_Paper/blob/master/read_esl/9-3and4/seki.png?raw=true)

基底関数同士が下の図のように、積になると両方の関数が0でない小さな部分だけが非ゼロになる。結果として、回帰面は本当に必要な部分だけ、非ゼロの要素を局所的に使って少しずつ構成される。
多項式の基底関数では非ゼロの領域があちこちにできてしまうので難しい。

また、計算量的にもこの基底関数の方が都合がよく、$O(N^2)$のところを$O(N)$になる。
(詳しくは本書を参照)

## 9.4.1 例: スパムメール (続き)
以前にも示したスパムメールにも適用する。結果として誤分類率は5.5%では足りで下げ止まり、前に述べた一般化加法的モデルの値(5.3%)よりわずかに大きい。

## 9.4.2 例: 試行データ
3種類のシナリオを作ってMARSを比較。結果として高次の交差項を含むシナリオ3で大きく性能を落としていた。

## 9.4.3 その他の話題

**MARSによる分類**

MARSのアルゴリズムは分類問題向けに拡張することもできる。二値分類は、出力を0か1で表して回帰問題に置き換えられる。多値分類は4.2節で述べた指示応答変数を用いればOK、と言いつつも問題があるので12.5節で議論する「最適スコア付け」法を用いるのが最適。

**MARSとCARTの関係**

MARSとCARTは一見かなり違って見えるが、モデル化の戦略は実は極めて似ている。MARSの手続きについて次の変更を行う。

1. 区分的線形な基底関数を階段関数$I(x-t \gt 0)$と$I(x-t\leq 0)$で置き換える。
2. モデルの項と候補の項との積を作ってモデルに含める時、元のモデル項がこの交差項で置きかわりその後の交差項には使えなくなる。

この変更を施すことで、MARSの構築アルゴリズムはCARTの木の成長アルゴリズムと同じになる。階段関数に他の対象な階段関数のペアをかけることは、頂点をそのステップで分割することに等しい。
2番目の制限から頂点は2回以上分割されないという制限になる。CARTはこの制限により、加法的な構造をモデル化することが難しくなっている。MARSは木構造を捨てることで、加法的な効果を得られている。

**混合された入力**

MARSは質的変数と量的変数を持つ混合された予測変数も自然に扱うことができる。
MARSは質的変数のカテゴリを2つのグループに分割する全ての場合を考え、各分割は2つのカテゴリ集合のどちらかに含まれるかを表す関数に対応する。その関数は、区分的な基底関数のペアであると置き換えられる。
このペアがこれまでの基底関数のペアと同様に扱われて、モデルに含まれる他の基底関数とのテンソル積を作るのに使われる。(どうやって積を作るんだろうか。。。)

