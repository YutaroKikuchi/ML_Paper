# 3.4 縮小推定
今までの変数選択 ← 変数を保持するか除外するかの2値での処理
→ 分散が大きくなることが多いため全変数モデルの予測誤差を減らせることは難しい
縮小推定:
連続的に変数を除外することで、この問題を解決
## 3.4.1 リッジ回帰
回帰係数の大きさに罰則を課すことで係数の値を縮小させる。

$$
\hat{\beta}^{ridge} = \argmin_{\beta}\{\displaystyle \Sigma^{N}_{i=1}(y_i-\beta_0- \displaystyle \Sigma^p_{j=1}x_{ij}\beta_i)^2 + \lambda\Sigma^p_{j=1}\beta_j^2\}
$$

最適な係数$\hat{\beta}^{ridge}$は罰則付き残差二乗和の最小化により求められる。
$\lambda$は複雑度パラメータであり、大きくなると縮小度合いも大きくなる。全ての係数は0に向かって縮小されるため、荷重減衰と言われる。
この式では切片$\beta_0$は罰則の項に含んでいない。これは変数選択が Yの原点の選び方に依存してしまうことを防ぐためである。

別な表現でのリッジ回帰が↓

$$
\hat{\beta}^{ridge} = \argmin_{\beta}\{\displaystyle \Sigma^{N}_{i=1}(y_i-\beta_0- \displaystyle \Sigma^p_{j=1}x_{ij}\beta_i)^2 \} \\

\text{subject to } \Sigma^p_{j=1}\beta_j^2\leq t
$$

既存の残差二乗和の最小化に制約を追加した。
この制約条件がなかった場合、係数の推定は不安定になりある。例えば、変数は不当に大きくなり別な変数は負の方向に大きくなることもありうる。

リッジ回帰の解は入力変数の大きさに対して不変ではないため、入力変数を標準化しておく必要がある。
そこで$x_ij$を$x_ij-\bar{x}$で中心化する。
加えて切片$^\beta_0$は$\bar{y}=\frac{1}{N}\Sigma^N y_i$から求めることで、切片を無視した係数を求めることができる。

入力の行列$\bf X$と目標のベクトル$\bf y$を元に上の式を行列形式で表す。

$$
(\bf y - \bf X \beta)^T(\bf y - \bf X \beta)+\lambda\beta^T\beta
$$

この時リッジ回帰の解は次のように求められる

 $$
 \hat{\beta}^{ridge} = (\bf X^T{\bf X}+\lambda\bf I)^{-1}\bf {X}^T{\bf y}
 $$


$N \times p$の行列$\bf X$に対して特異値分解を行うことで、リッジ回帰の特性の知見を得る。
$\bf X=\bf U\bf D{\bf V}^T$
