### 14.3.9 ベクトル量子化
K平均クラスタリング法は画像圧縮にも関係している。この説では*ベクトル量子化(Vector Quantization)*について解説する。
図14.9左の画像は画素あたり8ビット、全体で1メガバイトの容量が必要であるが、中央の画像は左の画像をVQを用いて圧縮したものであり、記憶容量は元画像の0.239倍で済んでいる。右の画像はさらに圧縮したものであり、記憶容量は元画像の0.0625倍しか必要としない。



ここでのVQでは1024×1024の画像を小さなブロック(この例では2×2画素のブロック)に分割、512×512の各ブロックの4つの値を長さ4のベクトルとみなす。K平均クラスタリング法をこの空間で実行する。
中央の画像はK=200クラス、右画像ではK=4クラスと設定している。
この文脈では、割り当てるクラスをコードワード、コードワードの集合をコードブック、クラスタリング処理のことを符号ステップと呼称する。

**記憶容量について**:
ブロックごとに各値を近似するコードワードを指定する必要があり、これには各ブロックで$log_2(K)$ビットが必要になる。さらに、コードブック自体のためにK×4個の実数が必要になる。(実際の計上では誤差)したがって、全体で圧縮画像の容量は元画像の$log_2(K)/(4*8)$倍となる。K=200なら$log_2(200)/(4*8)=0.239$でありK=4なら$log_2(4)/(4*8)=0.063$となる。一般的には$log_2(K)/4$をレートといい、圧縮の度合いを表す。

**復号について**:
上述の圧縮では、元画像が劣化したものが得られるので、非可逆圧縮と呼ばれる。劣化の度合いを歪みと呼び、二乗誤差で測る。上の例ではK=200の場合D=0.89、K=4の場合D=16.95となる。一般的にはレート/歪み曲線をそのトレードオフを評価するために用いる。

**量子化の改良**:
コードワードをコードブックから特定するには$log_2(K)$ビットが必要である。ここでは固定長符号を使っているが、あるコードワードが他のコードワードより頻出する場合、固定長だと非効率的である。可変長コードにすることによりさらなるレートの改善が見込める。

### 14.3.10 Kメドイドクラスタリング
K平均クラスタリングは、非類似度が二乗ユークリッド距離である場合に適しているが、
* 全ての変数が量的変数でなければならない。
* 二乗ユークリッド距離を用いることで最大距離がもっとも大きい影響力を持つことになり、ロバスト性が損なわれる。
といった欠点が存在する。これらの欠点を計算量を犠牲にすることで取り除くことができる。

K平均クラスタリングはクラスタの中心の座標を求めていたが、Kメドイドはクラスタのメドイド(クラスタ内のデータ点で，その点以外のクラスタ内の点でまでの非類似度の総和が最小になる点)を用いる。
座標ではなく、データ点を用いるため対応する非類似度行列などを用意すれば、量的変数である必要はない。さらに、非類似度は任意に設定できるため、ロバスト性も確保できている。

**計算量について**:
式14.32を解くには$\mathrm{O}(N_k)$で済んだが、式14.35を解くには$\mathrm{O}(N_k^2)$が必要となる。($N_k$は割り当てられたクラスタのサイズ)

#### 例:国の非類似度
12カ国の非類似度を学生へのアンケートに基づいて算出→3メドイドクラスタリングを行なっている。本書の図14.10の通り、変数が量的でなくてもいい感じに分類できている。

### 14.3.11 実用上の問題

K平均クラスタリングやKメドイドクラスタリングを適用するには、適切なクラスタ数$K^*$と初期値(中心となるデータ点を指す)を決めなければならない。

**クラスタ数$K^*$について**:
データに基づいて適切なクラスタ数$K^*$を推定する手法としてクラスタ内非類似度$W_K$をクラスタ数$K$の関数として調べるものがある。
$W_K$については詳しい説明はなかったが、おそらく式14.28のようにクラスタ内での非類似度の和を表すと考えてよいのだろう。
$K\in \{1,2,\cdots,K_{max}\}$のそれぞれでクラスタリングした時、クラスタのサイズが小さくなることから通常Kの増加とともに$\{W_1, W_2, \cdots, W_{K_{max}}\}$ の値は減少する。加えて、クラスタ内非類似度の差分$W_K - W_{K+1}$が$K=K^*$の時、急速に減少する傾向がある。

**直感的な説明**:
$K<K^*$かつアルゴリズムによって得られるクラスタには、真のグループが部分集合として含まれるという前提の時、クラスタ数を1つ増加させるごとに真のグループ内の観測は全て別のクラスタに分類されていくため、クラスタ内非類似度は急速に減少する。
$K>K^*$の時、推定クラスタの1つは少なくとも真のグループを分割することになる。この時点でKを増加させてもクラスタ内非類似度の減少は少ないことが見込める。
すなわち、クラスタ数と非類似度の相関をみて、日類似度が急速に減少するクラスタ数が適切なクラスタ数$K^*$である可能性が高い。



### 14.3.12 階層的クラスタリング

K平均およびKメドイドクラスタリング法の結果はクラスタ数および初期割り当てに依存する。一方階層的クラスタリングはそれらを設定する必要がない。代わりに2グループに分類したグループ間の非類似度を、観測点間の非類似度から算出してやる必要がある。

階層的クラスタリング法は凝集型(ボトムアップ)と分割型(トップダウン)の2種類に大別できる。
* 凝集型: 最下層からはじめ各階層で選択した2つのクラスタを1つのクラスタに再帰的に併合していく。もっとも小さい非類似度同士のクラスタを選択する。
* 分割型: 最上層から初めて各階層で既存のクラスタの内の1つを2つの新しいクラスタに再帰的に分割していく。そのグループ間日類似度が最大になるような2つの新たなグループを分割対象として選ぶ。

階層的クラリングのメリットとして、二分木でクラスタリングを表現できるデンドログラムが存在する。デンドログラムは、アルゴリズムの結果の記述よりもデータの視覚的な要約として用いられる。しかし、そのような解釈には注意が必要である。

デンドログラムによって、生成された階層構造がどれくらいデータを実際に表現しているかを示す指標として共表形相関係数が存在する。
入力である$N(N-1)/2$個の観測間非類似度$d_{ii^\prime}$と、デンドログラムから得られる共表形非類似度$C_{ii^\prime}$との相関係数。
共表形非類似度とは、観測$i$と$i^\prime$が初めて同じクラスたに入った時のグループ間非類似度である。
共表形相関係数が高いほど、うまくデータを分類できているということになる。

#### 凝集型クラスタリング
各観測がそれぞれ一つのクラスタを構成→もっとも非類似度が小さい2つのクラスタを1つに併合する。

**グループ間の非類似度の定義**:
グループ$G, H$のグループの非類似度$d(G,H)$を定義する。定義は以下の3種類がある。

* 単連結法
  * グループG,Hに所属している**もっとも非類似度が低い**観測ペアの非類似度を採用する。
$$
d_{SL}(G,H) = \min_{i\in G i^\prime \in H} d_{ii^\prime}
$$
* 完全連結凝集型クラスタリング
  * グループG,Hに所属している**もっとも非類似度が高い**観測ペアの非類似度を採用する。
$$
d_{CL}(G,H) = \max_{i\in G i^\prime \in H} d_{ii^\prime}
$$
* 群平均クラスタリング
  * グループ間の平均非類似度を用いる。
$$
d_{GA}(G,H) = \frac{1}{N_GN_H}\sum_{i\in G}\sum_{i^\prime \in H} d_{ii^\prime}
$$

単連結法は観測ペアの日類似度が小さいものが1つでもあれば、G,Hは近いとみなし、他の観測の非類似度は考慮しない。そのため、各クラスタ内の前観測が互いに似ている「コンパクト性」を満たさないことがある。
完全連結法は単連結と正反対の性質を持つ。小さいコンパクトなクラスタを生成しやすい傾向にある。しかし、あるクラスタに割り当てられた観測が、同じクラスタの観測よりも他のクラスタの観測に近くなることがある。

群平均法は両極端な上記2手法を折衷したものである。しかし、結果は非類似度$d_{ii^\prime}$を測る数値尺度に依存する。例えば、$d_{ii^\prime}$に単調増加関数$h(\cdot)$を適用し$h_{ii^\prime}=h(d_{ii^\prime})$を非類似度として用いると、結果が変わってしまう。一方単連結法や完全連結法は$d_{ii^\prime}$の順序のみに依存するため、不変性が保たれる。

#### 分割型クラスタリング
トップダウンで分割していく手法。凝集型ほど研究されてこなかったが、データを比較的少数のクラスタにクラスタリングしたいときに有用である。
全観測を1つのクラスタに配置→他の観測との平均非類似度が最も大きい観測をベースにしてもう1つのクラスタを生成する手法などが存在する。

