 # Learning to Assign Credit in Reinforcement Learning by Incorporating Abstract Relations

物体検知や位置推定などを挟んで、状態の次元を圧縮することで、探索コストを下げた手法。
現実に近いタスクはこの問題に悩まされがちなので、読んでみました。
適用環境はゲームに関する深層強化学習だけなので業務に完全に使うのは難しそう。

# Abstract
- Credit assignment problem(貢献度配分問題)は強化学習では非常に重要な問題である。
- 貢献度配分問題：
    - 報酬を獲得した場合、報酬の獲得にどの行動が効いたのかはエージェントにはわからない。
-   この問題は決定過程が幾千もの行動に関わってくる現実に近い環境ほど影響が強くなる。
- そこで、状態-行動間の貢献度を分配する計算過程を効率化することで、学習速度を向上させる。
-   具体的には、はじめに元の問題の状態と行動をより抽象化させることで、コンパクトな表現にする。これによって、元の問題を扱いやすいサイズにする。
-   そのあと、抽象化した問題に対して最適な価値関数を学習し、未来の価値を予測する。
-   最終的には、抽出した価値関数から、貢献度をを元の状態-行動ペアに割り当てる。
-   実験ではDoom環境に適用し、結果として、過去のエージェントに対してスコアに大きく差をつけた。
- エージェント同士を戦わせるコンペにも出場し、2位を獲得

# Introduction
- 貢献度分配問題は強化学習においてもっとも重大な挑戦である
- 一般的な環境では最終的な報酬は膨大な数の行動に依存しているため、この問題は非常に難しい。
- ほとんどの手法が、設定した期間の間の報酬を伝搬させることで無理矢理解決している。
- Atari2600のMontezma Revengeがいい例で、エージェントはマップ内の鍵を集めて扉を開ける必要があるのだが、そのためには、鍵がある画面にすべて貢献度を割り当てなければ行けないため、非常に実現性が低い。
- Reward shapingなどはエキスパート知識を組み合わせることで報酬関数に補正を加える、この問題を解決している。
    - しかし、エキスパート知識は概念であり、関数として扱うには難しい。
-   OpenAI FIVE は膨大なリソースを使ってreward shapingを行いDOTA2をプレイするエージェントを作ったが、人間には勝てていない。  
-   この問題を解決するため、貢献度分配問題のフレームワークを示す。
-   元の問題を抽象的な形に圧縮し、最適な価値関数を算出したあとに、元の問題の貢献度分配問題に適用していきます。
-   この方法によって、何十万もの状態-行動のペアの貢献度を自動的に割り当てることができるでしょう  

-   提案手法は3つの段階に分割される。抽象化、計画、フィードバックの3つである。
-   抽象化フェーズでは、First-Order-Logic(一階述語論理)を使って、各状態(オリジナル状態)をfirst-order-logic 命題(リレーショナル状態 )に抽象化する。複数のオリジナル状態は1つのリレーショナル状態にマッピングされるため、リレーショナル状態の数はオリジナル状態の数よりはるかに小さくなる。
-   計画フェーズでは、価値反復法で、各リレーショナル状態に対して、最適な価値関数を学習する。
-   前のフェーズで得られた最適な価値関数はオリジナル状態の価値関数ではない。そのため、potential functionを最適価値関数として導入する。これはreward shapingにも使われている手法である。

-   実験の環境としてFPSゲームのDoomを採用した。
-   オリジナルの報酬関数は敵を倒すことであるが、エージェントは複雑なマップ内を索敵し敵と戦闘し勝利してようやく得られる報酬であるため、報酬は疎であり、かつ遅れて得ることになる。  

-   構成要素ごとの比較、過去に発表されたエージェントとの対戦、実際に開催されたコンペティションのランキングを評価実験として示す。
-   貢献度分配の方法をいくつかのカテゴリに分類し、各方法のパフォーマンスを比較しました。
-   対戦では、過去に実装されたエージェントと戦わせた。その結果、キル数、デス数共に過去エージェントを上回った。
-   最後にDoom AI コンペティションに出場した結果、1位のエージェントと僅差の結果を獲得した。

  

# Related Work

## Deep Reinforcement Learning

-   ゲームを対象としたDRL研究が盛んに行われている。
-   エージェントは目的の状態に到達するために適切な行動を学習する。
-   代表的な成果といえば、3Dゲームを対象としたチャレンジであり、 Deep Recurrent Q-learningやカリキュラム学習が有名。
-   にも関わらず、いくつものチャレンジが計算量の都合上、残っている

## Relational Reinforcement Learning

-   Relational Reinforcement Learningは強化学習と Relational Learningを組み合わせたもので、状態、行動、Q関数を表現力の高い言語で示していることから内部が構造的なタスクに効果的である。
-   ZambaldiらによるDRRLはエンティティ間の関係を推論するためにself-attensionを使用して、モデルに依存しないポリシーを学習した。
-   本稿では、RRLの技術を導入し、よりコンパクトな問題の表現を試みる。

# Method

-   Markov decision Processを前提におく。
-   新たな報酬関数を定義するのに必要なので、MDPを拡張したRelational MDPを考える。
-   RMDPの主なアドバンテージはエキスパート知識を使って報酬関数を設計するのが簡単であるということである。
-   RMDPでの報酬関数R_rを次のように定義する
-   /*RMDPでの報酬関数の定義式*/
-   V_rはRMDPの状態に対する最適価値関数、fは各状態をRMDPにマッピングする関数。

  

## Abstraction

-   元の状態を入力として、一階述語論理に変換して出力する。
-   数式としては次のように表す。
$S_r = f_s(S), A_r = f_a(A)$
- 
-   RDMPでは必要な情報のみを保存する。
-   例えば画像に関するタスクの場合、背景情報を削除することで前景の情報のみを保存する。
-   この段階では複数の状態を1つの状態として表すので、状態数の削減が期待できる。

- この実装は元のタスクの入力形式や保持したい情報の種類によって異なる。
- シンプルなグリッドゲームの場合はルールベースの変換で十分だが、視覚情報を使うタスクでは、物体検知やImage segmentationなどが有効である。

- 抽象化の例
	1. 入力の画像: 箱の上にリンゴが乗っかっている。
	2. 物体検知でリンゴと箱の位置をそれぞれ推定。
	3. 座標位置から$on(apple, box)$をRMDPの状態として変換。
	4. 行動として予め定義していた述語($move(x,y)など$)を各行動にマッピングする。

## Planning
- もしMDPのパラメータが全てわかっているなら、強化学習の問題は計画問題と等価である。
- この段階では演算方法として$T_r$とリプレイデータから得られるパラメータを取得する。
- RDMPでの報酬関数はMDPでの報酬関数の部分集合となる。MDPにおける全ての非ゼロの状態-行動のペアはRMDPでも保持される。
- このように変換したRMDPを、価値反復アルゴリズムによって解き、最適な価値関数を獲得する。
- 最適価値関数はバックプロパゲーションで学習するので、

# Application
- Doomを対象にして実験を行う。
- Doomを選んだ理由：
	- 貢献度分配問題の影響を非常に受けやすいゲームであるためである。
	- オリジナルの報酬は敵を倒すことで得られるため、報酬を得られる頻度がとても少ない。
	- そのことから、どの行動が報酬に効くのかの判定がとても重要になってくる。

## Architecture
- Doomにおける提案手法とA3Cを組み合わせたフレームワークが↓
![fig2](fig2.png)
- 提案手法はDRLアルゴリズムに依存しないため今回はDoom上で実績があるA3Cを採用している。
- 画面下
- 画像上
    - 提案手法の抽象化と論理演算の実装を示している。
    - 抽象化には、物体検知と場所検知を行い、ゲーム内のオブジェクトのbounding boxとオブジェクト間の位置情報を予測してい る。
    - 抽出されたオブジェクトの情報は、予め定義された述語に渡され、次の状態が決まる。
    - RMDPのパラメータは全て決まっているため、オフラインの計画アルゴリズムで最適価値は導き出される。
    - 
# Evaluation

# Conclusion

> Written with [StackEdit](https://stackedit.io/).