# Unsupervised learning of object structure and dynamics from videos

# Introduction

動画データにおけるオブジェクトのトラッキングデータの抽出を教師なし学習のモデルによって実現する。

教師なしの動画理解のアプローチの1つとして、未来のフレームを予測することである。この方法には大きく2つの問題点がある。
1. 未来のフレームを予測することは確率的な予測になるため、ピクセルレベルの誤差で予測の良し悪しを測ることは非常に難しい。
   1. 意味的にとるに足らない誤差であっても、ピクセル単位の誤差にすると不当に大きくなってしまう。
2. 仮にピクセル単位で正確な予測ができるようになったとしても、最終的なタスクに対する効果的なソリューションにならない。
   1. 正確な再構築画像を入力しても、オブジェクトの動きのトラッキングには役に立たないことは自明。

提案手法はピクセルではなく、キーポイント表現を用いてこの問題に取り組む。キーポイントは顔・ポーズのトラッキングに用いられる、特徴点の一種である。通常キーポイントの検知器の学習には教師データが必要であるが、本手法では動画データのみを用いてキーポイント表現を学習する。この手法ではオートエンコーダを採用しており、最初に個々のフレームをキーポイントにエンコードし、キーポイントを別の動作モデルに入力することで、未来の動きを予測する。

すなわち、未来の画像の再構築と動作の予測モデルを分離しているのである。結果として、動作モデルを画像空間上で表現しつつ、画像再構成用のモデルはピクセル単位の再構成誤差だけで学習させることが可能になった。

実験結果として、既存手法を超えるスコアを示すと同時に下流のタスクに転用する際の有用性についても示している。


# 3.Architecture

提案モデルは、キーポイント検知器(keypoint detector)と動作モデルの2つに分けられる。検知器は各フレームを低次元なキーポイント表現にエンコードし、動作モデルは未来の動作をキーポイント座標で予測する。

## 3.1 unsupervised keypoint detector

**画像→キーポイントへの変換**
$\phi^{det}(\bf{v_t}) = \bf{x_t}$: 
$\phi^{det}$は各フレームから$K$個(キーポイントの個数)の特徴マップにエンコードする。
各特徴マップは正規化され、特徴マップの空間的期待値を元に(x,y)座標の表現となる。

**画像の再構築**
$\phi^{rec}(\bf{v_1}, \bf{x_t})$は、最初のフレームとキーポイントの集合から$\bf{v_t}$を再構築するジェネレータである。事前に$\phi^{det}$と$\phi^{rec}$を、中間層がキーポイント表現になるように学習させる。
各キーポイントはキーポイントの位置にガウシアン型の模様をもつマップに変換され、$\bf{v_1}$と結合する。

図示されていないが、

## 3.2 Stochastic dynamics model

動作モデルは時刻$t$のキーポイント座標$\mathbf{x}_t$から、オートエンコーダによって$\hat{\mathbf{x}_t}$を再構成することで、未来のキーポイントと考えている。単純にVAEを用いるだけではなく、RNNを取り入れたVariational Recurrent Neural Network(VRNN: VAEとRNNを組み合わせたオートエンコーダ)を採用することで、時系列反映している。
VRNNの参考(https://www.ogis-ri.co.jp/rad/webmaga/1265288_6728.html)

動作モデル内には、潜在変数である$\mathbf{z}_t$とLSTMの内部情報である$\mathbf{h}_t$が存在する。

時刻$t$以前のLSTMの内部状態$h_{t-1}$から時刻$t$以前の潜在変数$z_t^{prior}$($\phi^{prior}$の部分)を導出する。したがって全結合層$\phi ^{prior}$か時刻$t$以前の動きを表す$\mathbf{z_t}$が得られる。
$$
p(\mathbf{z_t} | \mathbf{x}_{\lt t}, \mathbf{z}_{\lt t}) = \phi ^{prior}(\bf{\mathbf{h}_{t-1}})
$$

全結合層$\phi ^{enc}$によって、時刻$t$における潜在変数$\mathbf{z}_t^{posterior}$を導出する。
$$
q(\bf{z_t} | x_{\leq t}, z_{\lt t}) = \phi ^{enc}(\bf{\mathbf{h_{t-1}}_{t-1}}, \mathbf{x}_t)
$$

$\mathbf{z}_t^{posterior}$からサンプリングした結果を$\phi^{dec}$に入力し、$\hat{\mathbf{x}_t}$を再構成する。

$$
p(\mathbf{x}_t |z_{\leq t}, x_{\lt t}) = \phi ^{dec}(\mathbf{z}_t, \mathbf{h}_{t-1})
$$

最後に内部状態$h_{t-1}$を$\phi^{RNN}$でアップデートする。
$$
\mathbf{h}_t = \phi^{RNN}(\mathbf{x}_t, \mathbf{z}_t, \mathbf{h}_{t-1})
$$

# 5 Results

実験結果として、2つのデータセットに対して、予測の品質が向上したことと、フレーム内のオブジェクトレベルの情報が必要な下流のタスクに有用であるということを示す。

## 5.1 Structured representation improves video prediction

使用したデータセット：
* バスケットボールデータ：バスケコートを真上から見下ろした動画。五人のプレイヤーはカラーのドットで表される。複数のオブジェクトの複雑な動作の検知をテスト。
* Human3.6 : 1人の人間が様々な動きをする動画。動作の検知をテストする。

両データセットは評価用として、正解の動作データを持っている。(モデルには入力しない)

結果比較として、条件を変えた提案手法4つと既存手法3つに対して比較を行った。(fig3)

fig3上部の画像群はかく実験条件ごとに再構成画像の比較を行っている。時間が経つごとに予測の精度が落ちていることが確認できる。しかし、Struct-VRNN(提案手法を全て盛り込んだ条件)のfutherestの画像と他条件のclosestを比較すると、Struct-VRNNが最もよく再構成できていると言える。さらに、Struct-VRNNはfurtherestであっても四肢の再現度は高いことから、多様で高品質な再構成ができていることが示唆される。

fig3下部の左右のグラフはそれぞれ、コサイン類似度(高いほど良い)・フレシュのビデオ距離(FVD:低いほどよい)を表している。画像1枚ごとの評価を行うコサイン類似度での比較では既存手法より劣っていると解釈できる。一方FVDは生成モデルの確率分布の評価方法であるため、動画全体の再構成品質という背景の場合、Struct-VRNNが既存手法より優れていると考えられる。

# 5.2 The learned keypoints track objects

本提案モデルはキーポイントとオブジェクトの動きを同時に学習しているため、キーポイントを直接操作して再構成画像に反映させることが可能である。
例として、fig7のキーポイントを操作した例を示す。fig7上では手足のキーポイントをそれぞれ動かし、任意の部分だけを移動させている。


# 5.5 Structured representation retains more semantic information

