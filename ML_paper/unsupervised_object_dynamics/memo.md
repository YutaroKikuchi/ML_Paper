# Unsupervised learning of object structure and dynamics from videos

# 著者
Matthias Minderer∗ Chen Sun Ruben Villegas Forrester Cole Kevin Murphy Honglak Lee 
Google Research

Neur IPS 2019


# サマリ

* 教師データなしで、動画内のオブジェクトの動きを認識・自然に未来のフレームを予測することができるモデルを提案。
* 既存手法より良いスコアを獲得
* 提案手法はより下流のタスクに応用可能であることを示唆

# Introduction
対象となる問題
* 動画データにおけるオブジェクトのトラッキングデータの抽出
この問題を教師なし学習のモデルによって実現する。

上の問題に取り組むにあたって、未来の動画フレームを予測することが有効なアプローチである。このアプローチには大きく2つの問題点がある。
1. 未来のフレームを予測することは確率的な予測になるため、ピクセルレベルの誤差で予測の良し悪しを測ることは非常に難しい。
   1. 意味的にとるに足らない誤差であっても、ピクセル単位の誤差にすると不当に大きくなってしまう。
2. 仮にピクセル単位で正確な予測ができるようになったとしても、最終的なタスクに対する効果的なソリューションにならない。
   1. 正確な再構築画像を入力しても、オブジェクトの動きのトラッキングには役に立たないことは自明。

提案手法はピクセルではなく、キーポイント表現を用いてこの問題に取り組む。キーポイントは顔・ポーズのトラッキングに用いられる、特徴点の一種である。通常キーポイントの検知器の学習には教師データが必要であるが、本手法では動画データのみを用いてキーポイント表現を学習する。

さらに、抽出したキーポイントを動作予測モデルに入力することで、オブジェクトの未来のキーポイントを予測して、最終的に未来の画像を生成する。

すなわち、キーポイントの検知・動作の予測モデルで分離しているのである。結果として、画像再構成用のモデルはピクセル単位の再構成誤差だけで学習させることが可能になった。

実験結果として、既存手法を超えるスコアを示すと同時に下流のタスクに転用する際の有用性についても示している。

# 3.Architecture

提案モデルは、キーポイント検知器(keypoint detector)と動作モデルの2つに分けられる。検知器は各フレームを低次元なキーポイント表現にエンコードし、動作モデルは未来の動作をキーポイント座標で予測する。

![fig1](https://github.com/talesofyousan/ML_Paper/blob/master/ML_paper/unsupervised_object_dynamics/fig1.png?raw=true)


## 3.1 unsupervised keypoint detector

**画像→キーポイントへの変換**

$\phi^{det}(\bf{v_t}) = \bf{x_t}$: 
$\phi^{det}$は各フレームから$K$個(キーポイントの個数)の特徴マップにエンコードする。
各特徴マップは正規化され、特徴マップの空間的期待値(おそらくウエイトが大きい座標の平均をとっていると思われる)を元に(x,y)座標の表現となる。

**画像の再構築**

$\phi^{rec}(\bf{v_1}, \bf{x_t})$は、最初のフレームとキーポイントの集合から$\bf{v_t}$を再構築するジェネレータである。事前に$\phi^{det}$と$\phi^{rec}$を、中間層がキーポイント表現になるように学習させる。

各キーポイントはキーポイントの位置を中心としたガウシアン型のフィルタに変換され、キーポイントヒートマップとなる。

ジェネレータに入力する際には、$\bf{v_1}$の特徴マップとキーポイントヒートマップを結合して入力することで$\hat{\mathbf{v}}_t$を再構成する


$\mu_k$はintensity(強度:おそらくはRGBの値から反映される数値)を表したもので、キーポイントの存在・重要度を表す。

## 3.2 Stochastic dynamics model

動作モデルは時刻$t$のキーポイント座標$\mathbf{x}_t$から、オートエンコーダによって$\hat{\mathbf{x}_t}$を再構成することで、未来のキーポイントと考えている。単純にVAEを用いるだけではなく、RNNを取り入れたVariational Recurrent Neural Network(VRNN: VAEとRNNを組み合わせたオートエンコーダ)を用いることで、時系列を表現している。

VRNNの参考(https://www.ogis-ri.co.jp/rad/webmaga/1265288_6728.html)

動作モデル内には、潜在変数である$\mathbf{z}_t$とLSTMの内部情報である$\mathbf{h}_t$が存在する。

**$\phi^{prior}$の部分**

時刻$t$以前のLSTMの内部状態$h_{t-1}$から時刻$t$以前の潜在変数$z_t^{prior}$を導出する。したがって全結合層$\phi ^{prior}$から$x_t$を入力する前の潜在変数$\mathbf{z_t}$が得られる。
$$
p(\mathbf{z_t} | \mathbf{x}_{\lt t}, \mathbf{z}_{\lt t}) = \phi ^{prior}(\bf{\mathbf{h}_{t-1}})
$$

**$\phi ^{enc}$の部分**

全結合層$\phi ^{enc}$によって、時刻$t$における潜在変数$\mathbf{z}_t^{posterior}$を導出する。
$$
q(\bf{z_t} | x_{\leq t}, z_{\lt t}) = \phi ^{enc}(\bf{\mathbf{h}_{t-1}}, \mathbf{x}_t)
$$

**$\phi^{dec}$の部分**

$\mathbf{z}_t^{posterior}$からサンプリングした結果を$\phi^{dec}$に入力し、$\hat{\mathbf{x}_t}$を再構成する。

$$
p(\mathbf{x}_t |z_{\leq t}, x_{\lt t}) = \phi ^{dec}(\mathbf{z}_t, \mathbf{h}_{t-1})
$$

**$\phi^{RNN}$の部分**

最後に内部状態$h_{t-1}$を$\phi^{RNN}$でアップデートする。
$$
\mathbf{h}_t = \phi^{RNN}(\mathbf{x}_t, \mathbf{z}_t, \mathbf{h}_{t-1})
$$
この時$x_t$は$z_t^{prior}$から$\phi^{dec}$で生成した座標を入力することができるので、$\mathbf{x}_t$を観測せずに、生成した座標から遠い未来の動きを予知できるのである。

# Training

## 4.1 keypoint detector

キーポイントの検出器には単純なL2画像再構成誤差の他に、キーポイントの数を減らすために追加のロス関数を用意した。

* 時間的な分離損失
  * 動きの相関性が高い特徴点は同じオブジェクトを表している可能性が高い。そこで、キーポイントが時間的に相関しないように、時刻1から$T$までの各キーポイント同士のユークリッド距離が近いとペナルティを課すよう損失関数を定義した。
* キーポイントのスパーシティ損失
  * キーポイントの密度にペナルティをかける。キーポイントのスケールである$\mu$のL1誤差をとる。


# 5 Results

実験結果として、2つのデータセットに対して、予測の品質が向上したことと、フレーム内のオブジェクトレベルの情報が必要な下流のタスクに有用であるということを示す。

## 5.1 Structured representation improves video prediction

使用したデータセット：
* バスケットボールデータ：バスケコートを真上から見下ろした動画。五人のプレイヤーはカラーのドットで表される。複数のオブジェクトの複雑な動作の検知をテスト。
* Human3.6 : 1人の人間が様々な動きをする動画。動作の検知をテストする。

両データセットは評価用として、正解の動作データを持っている。(モデルには入力しない)

結果比較として、条件を変えた提案手法4つと既存手法3つに対して比較を行った。(fig3)

![fig3](https://github.com/talesofyousan/ML_Paper/blob/master/ML_paper/unsupervised_object_dynamics/fig3.png?raw=true)

fig3上部の画像群は各実験条件ごとに再構成画像の比較を行っている。時間が経つごとに予測の精度が落ちていることが確認できる。しかし、Struct-VRNN(提案手法を全て盛り込んだ条件)のfutherestの画像と他条件のclosestを比較すると、Struct-VRNNが最もよく再構成できていると言える。さらに、Struct-VRNNはfurtherestであっても四肢の再現度は高いことから、多様で高品質な再構成ができていることが示唆される。

fig3下部の左右のグラフはそれぞれ、コサイン類似度(高いほど良い)・フレシュのビデオ距離(FVD:低いほどよい)を表している。画像1枚ごとの評価を行うコサイン類似度での比較では既存手法より劣っていると解釈できる。一方FVDは生成モデルの確率分布の評価方法であるため、動画全体の再構成品質という背景の場合、Struct-VRNNが既存手法より優れていると考えられる。

# 5.2 The learned keypoints track objects

本提案モデルはキーポイントとオブジェクトの動きを同時に学習しているため、キーポイントを直接操作して再構成画像に反映させることが可能である。
例として、fig7のキーポイントを操作した例を示す。fig7上では手足のキーポイントをそれぞれ動かし、任意の部分だけを移動させている。

![fig7](https://github.com/talesofyousan/ML_Paper/blob/master/ML_paper/unsupervised_object_dynamics/fig7.png?raw=true)

# 5.5 Structured representation retains more semantic information

学習したキーポイントはアクションの認識や強化学習の報酬の予測など、下流のタスクに適用することができる。

アクション認識のパフォーマンスを試すため、Human3.6Mのテストデータを使ったアクション認識タスクの精度を比較した。その結果、キーポイントを画像座標上に置き換えた条件とそうでない条件とで精度に

DMCSを使った報酬の推定を行った。画像と報酬のデータセットはDMCSの環境上でランダムな行動をとって集めた。モデルは、エージェントのアクションの条件の予測をするため、アクションをRNNの追加のタスクとした。モデルはタスクの報酬関数にはアクセスできない。


# 6 Discussion

提案したキーポイントの空間表現は、非構造的表現と明示的にオブジェクトと紐付けた表現との中間的存在である。

* キーポイントを画像空間上に落とし込んだ表現を採用したことで、他の下流タスクに応用が可能に。様々な問題でSOTAを獲得している。
* オブジェクトマスクや奥行きなどメタ情報に寄せたモデル構造にしていない。そのため、様々な形のオブジェクトで安定した訓練を実現している。

# 所感

* 特徴表現を画像座標軸に落とし込むアイデアは興味深かった。
* 案件でも応用先が見込めそうな内容だった。
  * 実装も上がっているため、手軽に試せそうではある。
  * [github](https://github.com/google-research/google-research/tree/master/video_structure)
* 実際に適用されている例も複数あった
  * Object tracking : 人混みのなか人間をラベリングしている。(https://paperswithcode.com/task/object-tracking)
  * Video prediction: 動画の未来のフレームを予測。(https://paperswithcode.com/task/video-prediction)
  * Continuous Control: 説明なし (https://paperswithcode.com/task/continuous-control)
