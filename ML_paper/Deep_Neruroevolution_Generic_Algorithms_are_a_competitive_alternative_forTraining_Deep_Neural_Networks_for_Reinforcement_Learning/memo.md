# Introduction

シンプルな遺伝的アルゴリズムをDRLに適用してみた。
Atari2600とHumanoid Locomotionで実験してみた。
DQNとA3Cと比較
はじめはうまくいかないと思われたがいくつかのドメインで既存手法を上回ることに成功
ランダムサーチよりもパフォーマンスがよくなることを示す

いくつかのゲームで良いパフォーマンスがよかった。
このことは、局所最適、鞍点、勾配のノイズなどが関係していると思われる

# Background
RLとDRLの話

DQNとA3Cの話

DNNの勾配を近似または演算し確率的に最適化をしていることから、DQNとA3Cは勾配ベースの手法である。
それに対してESは解析的に勾配を算出することはなく、代わりに報酬関数の勾配を近似する。

# Methods

## Generic Algorithm
はじめにシンプルなGAをベースラインとして、実装し、既存のGAのアルゴリズムを導入し、パフォーマンス向上を確認する。

GAはN個の個体を含む集団Pを進化させるアルゴリズムである。
ここでは個体をNNのパラメータ$\theta$と考える。
適応関数$F(\theta_i)$
truncation-selectionを使用。適応値が上位T個の個体を選び次の世代の親とする。
次の盛大を生み出すため、次のプロセスを$N-1$回繰り返す。
1. 親は一様ランダムに選択されgausiannノイズによって変異する。
2. $N$番目の個体は変異せず、前の世代でのベストな個体が採用される。

ソースコード：https://github.com/uber-research/deep-neuroevolution

GAは通常、各個体を保存しておくのが普通だが、非常にメモリを食うため、今回は不採用としている。
その代わりより多くのパラメータを保存できる方法を提案する。
初期シードと各変異のランダムシードを保存しておくことで、再現できるようにしている。

この圧縮した表現は世代に対して線形に増加していくだけなのでリーズナブルかつネットワークのサイズに依存しない
ただしDNNの再構築には計算時間がかかるが
ネットワークの圧縮ではSOTA並みの成果を得られたが残念ながらGA以外に適用するのは困難

## Novelty Search
GAによるDNNトレーニングは、利益をもたらす。
デモンストレーションとして、novelty searchにて実験する。
NSは局所最適に陥るようなドメイン向けに設計されている。
NSは進化中は報酬を無視し、代わりに未知の動作に遭遇したら報酬鵜を与える。
驚くことに報酬を利用するアルゴリズムより、良いスコアを出している。

NSでは方策からなる振る舞い特性(BC)$BC(\pi)$を定義する必要がある。BCの距離関数も定義する。
各世代について、個体はそれぞれ、それらのBCがarchiveに格納される確率$p$を持っている。
方策の新規性は$k$個の最近傍点の距離の平均から算出される。

# Experiments

## Atari
DNN+GA versus DQN, A3C, ESを比較