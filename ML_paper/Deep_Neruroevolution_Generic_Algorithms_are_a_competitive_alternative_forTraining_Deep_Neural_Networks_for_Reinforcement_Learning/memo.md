# Introduction

シンプルな遺伝的アルゴリズムをDeep Reinforcement Learningに適用してみた。

実験対象:
* Atari2600: 2Dアクションゲーム
* Humanoid Locomotion: 二足歩行シミュレータ

比較対象:
* Deep Q Network: 
* Asyncronous Advantage Actor-Critic:

はじめはうまくいかないと思われたがいくつかのドメインで既存手法を上回ることに成功
ランダムサーチよりもパフォーマンスがよくなることを示す。
このことは、局所最適、鞍点、勾配のノイズなどが関係していると思われる。

# 

# Background
DQNなどの価値反復法やA3Cなどの方策勾配法による深層強化学習の発展の中、Salimanらによって、Evolution Strategyアルゴリズム(以下 ES)を用いた手法も提案されている。このESアルゴリズムはパラメータ(←何の？)の分布の平均は学習できるものの、分散までは学習できなかった。とはいえ、適切に複数のCPUを用いた並列化処理を行うことで、wall-clock上では少ない時間で学習はできている。

DNNの勾配を近似または演算し確率的に最適化をしていることから、DQNとA3Cは勾配ベースの手法である
それに対してESは解析的に勾配を算出することはなく、代わりに報酬関数の勾配を近似する。

# Methods

## Generic Algorithm
この実験ではシンプルな遺伝的アルゴリズムをベースラインとして実装し、次節で既存のテクニックを導入し、パフォーマンス向上を確認する。今後、さらにシンプルなGAに改良を加えることで、さらなる性能向上が期待できる。

遺伝的アルゴリズムは$N$個の個体を持つ集団$P$を進化させるアルゴリズムである。ここでは個体をNNのパラメータ$\theta$と考える。
各generationにおいて、$\theta_i$は適応関数$F(\theta_i)$が向上するように進化してゆく。

適応値が高い上位$T$個の個体を選び次の世代の親とする。

遺伝的アルゴリズムは次の操作を進化として$G$回繰り返す。
1. 次の世代を生み出すため、次のプロセスを$N-1$回繰り返す。
    1. 前の世代から選ばれた親はランダムに選択されgausiannノイズによって変異する。
2. $N$番目の個体は変異せず、前の世代でのベストな個体を採用。
3. 変異した個体の中で$F(\theta)$が高い個体$T$個を次世代の親とする。

擬似コード
![](./simple_ga.png "hoge")

ソースコード：https://github.com/uber-research/deep-neuroevolution

GAは通常、各個体であるNNのパラメータ$\theta$を保存しておくのが普通だが、非常にメモリを食うため、単純な実装は無理。
そこで、パラメータの圧縮方法を提案する。
初期シードと各変異のランダムシードを保存しておくことで、再現できるようにしている。
この圧縮した表現は世代に対して線形に増加していくだけなのでリーズナブルかつネットワークのサイズに依存しない。ただしDNNの再構築には計算時間がかかる。

以下が概要図

![](./enc_params.png "hoge")
パラメータの初期化関数$\phi$とネットワークの変異関数$\psi$はどちらもシード値$\tau$で決定するため、シード値だけを保持しておけば、全世代の個体を再構成することが可能。

ネットワークの圧縮ではSOTA並みの成果を得られたが残念ながらGA以外に適用するのは困難。

## Novelty Search
前節のシンプルなGAを、前提案された手法を用いて改良する。
この論文ではNovelty Searchを適用する。NSは報酬ベースで最適化をしようとすると、局所最適に陥るような意地悪なドメイン向けに設計されている。
NSは進化中は報酬を無視し、代わりに未知の動作に遭遇したら報酬を与える。報酬を利用するアルゴリズムより良いスコアを出しやすい。

前説の遺伝的アルゴリズムとの差分は以下の通り。
* 方策によって決まる振る舞い特性(Behavor Characteristic)$BC(\pi(\theta))$を定義する。
* 適応関数$F(\theta_i)$は、BCの距離関数$dist(BC(\pi_i, \pi_j))$に変更。
* 距離関数はドメインによって異なる。

以下が擬似コード

![](./NA.png "hoge")

# Experiments

## Atari
提案手法とDQN、A3C、SalmanのESを適用してパフォーマンスを比較する。
* 計算速度、サンプル効率のトレードオフは不均一であるため、アルゴリズム同士の厳密な比較は難しい
* 他にも、パラメータの初期状態はランダムに決めるか、人間のプレイから決めるか。ここではランダムに決めている。
* DQNとESはランダムスタートだが、A3Cはヒューマンスタートである。
* トレーニング中のフレーム制限も考慮する。

結果として、13ゲーム中3つのゲームで既存手法以上のスコアを獲得できた。
特にSkiingでは実験対象外ではあるがRainbow DQNといった手法にも勝利している。

GAアルゴリズムはさらにパフォーマンスを向上できることを示すためさらに6倍フレームを使って学習させた。その結果、全てのゲームでスコアが向上した。

目覚ましい成果として、遺伝的アルゴリズムの成長速度である。集団のサイズを大きくしていたことから、世代数は
大きくならなかったことが原因であると考えられる。

