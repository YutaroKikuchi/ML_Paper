# Never Give Up : Learning Directred Exploration Strategies
Adrià Puigdomènech Badia∗ ら
DeepMind
ICLR2020 に採用

強化学習業界で大きな課題として、報酬が疎な環境での学習があげられる。報酬が疎な環境とは報酬を得られる頻度が少ないため、膨大な状態の探索を行わないとエージェントは正の報酬が得られる行動を経験・学習することができないという問題である。

強化学習の有名な環境として、Atari2600という環境が存在する。これは低解像度の2次元アクションゲームであり、強化学習のベンチマークとされてきた。このAtari2600のうち、いくつかのゲームは報酬が疎な環境となっており、長年人間の獲得スコアを超えることは難しいとされてきた。

本手法Never Give Up(NGU)では、報酬が疎な環境向けの学習手法を提案する。行なったことは以下の3点。

1. エピソード内における新規性とエピソードを跨いだ新規性によって変化する報酬の補正(NeverGiveUpの由来)
2. 探索と活用が入り混じった複数のポリシーの学習
3. 探索が困難なゲームにおけるstate-of-the-artと同等かそれ以上の獲得スコア

# THE NEVER-GIVE-UP INTRINSIC REWARD
時刻$t$における報酬$r_t$を次の2つに分割する。
$$
r_t = r_t^e + \beta r_t^i
$$
* $r_t^e$: 既存の報酬と同様。環境から与えられる報酬。**外部報酬**と呼称する
* $r_t^i$: エージェントが到達する状態の新規性により与えられるボーナス。**内部報酬**と呼称する。
* $\beta$: 内部報酬と外部報酬のバランスを調整するスカラー

通常RLは外部報酬のみで学習するが本手法では内部報酬を加えることで、エージェントに次のような性質を加える。

**内部報酬の性質**:

* 同じエピソード内で状態を再訪することを強く抑える
  * 短期的には同じ失敗を繰り返さない
* エピソードをまたいで1つの状態に何度も再訪することを控えめに抑える。
  * 長期的には失敗した状態でも再挑戦して別な行動を試してほしい
* 状態のうち、エージェントのアクションに依存せず変動する要素は無視する。
  * エージェントの行動に関係せずに変化する状態は新規性とは違うため

※ 観測された状態のうち、エージェントの行動に依存して変化する要素を抽出した状態のことを**制御可能な状態と呼称する。**

次のfig1にアーキテクチャを示す。

!['fig1'](https://github.com/talesofyousan/ML_Paper/blob/master/ML_paper/never_give_up/img/reward.png?raw=true)

内部報酬は2つのブロックからなり、短期記憶モジュール(赤いブロック)と長期記憶モジュール(緑のブロック, optimal)である。

この2つブロックで、内部報酬$r_t^i$は次の式で計算される。

$$
r_t^i = r_t^{episodic}\cdot \min\{\max\{\alpha_t, 1\}, L\}
\tag{1}
$$
$r_t^{episodic}$は短期記憶モジュールによって算出される。$\alpha_t$は長期記憶モジュールによって算出される。$L$は係数を抑えるためのハイパーパラメータ(デフォルトは5)。

### 短期記憶モジュールについて
エピソード内に訪れた状態をスタックするメモリ$M$と、状態となる画像の埋め込み関数$f$から成る。

**埋め込み関数$f$について**：

$f$は観測した状態から制御可能な状態を抽出し$p$次元のベクトルに写像する関数であり、**状態がエージェントと独立して変化する要素を排除することが目的**。
エージェントの学習前に次のfig2のようなCNNを学習する。

!['fig2'](https://github.com/talesofyousan/ML_Paper/blob/master/ML_paper/never_give_up/img/embedding.png?raw=true)

上図は、状態$x_t$と状態$x_{t+1}$を入力として、状態を遷移するために選択した行動$a$を予測する分類器である。
エージェントの学習前にこのCNNを学習させることで、行動$a$にだけ関係する要素だけ取り出せるという過程の下、埋め込み関数となるネットワークを埋め込み関数$f$として用いる。

**短期記憶メモリ$M$について**：

エピソードごとに空になるスタックメモリ。エピソードないで訪れた状態を全て保存する。

$x_t$が与えられた時、理論上最適とされる内部報酬はStrehl&Littmanらによって次のように導出されており、本手法もこの通りに従う。
$$
r_t^{episodic} = \frac{1}{\sqrt{n(f(x_t))}}
\tag{2}
$$
$n(f(x_t))$はメモリ$M$において訪れたことのある制御可能な状態$f(x_t)$の回数を意味している。つまり、エピソード内で未知の状態に到達したなら、報酬が大きくなる。

実装上は計算量の都合上、カーネルを使った方法で近似している。

$$
\frac{1}{\sqrt{n(f(x_t))}} \approx \frac{1}{\sqrt{\sum_{f_i}K(f(x_t), f_i)} + c}
\tag{3}
$$

$f_i$はM内の$i$番目に格納されている制御可能な状態。$K(f(x_t), f_i)$はベクトルをスカラーに写像するカーネル関数。$c$は分母0を防ぐための微小なスカラー。

上記の近似に加えて、$f(x_t)$の近傍$k$個の制御可能な状態のみを対象にすることで、計算量を節約している。

ちなみに、先駆者に習い、Kはinverse kernelを用いている。

$$
K(x,y) = \frac{\epsilon}{\frac{d^2(x,y)}{d^2_m} + \epsilon}
$$
$\epsilon$は微小な定数。$d$はユークリッド距離、$d^2_m$は$k$番目の状態の移動平均を表す。移動平均を付け加えることで、よりユークリッド距離の変動にロバストになる。

### 長期記憶モジュールについて

長期記憶モジュールはエピソード間における状態の新規性を示する$\alpha_t$を算出する。$\alpha_t$を状態ごとに制御することで、エピソードを跨いだ新規性を内部報酬$r_t^i$に反映させることができる。

ネットワーク$\hat{g}$は状態をランダムな$k$次元のベクトルに写像する。ウエイトは更新されない。$g$も状態を$k$次元のベクトルに写像する。エージェントの学習中に$\hat{g}$の出力を模倣するように学習するネットワークである。$g$の誤差は$err(x_t) = \|g(x_t;\theta) - \hat{g}(x_t)\|^2$となり、$\alpha = 1+\frac{err(x_t)-\mu_e}{\delta_e}$となる。エージェントの学習が進む毎に、エピソード間で再訪する状態が増える→$err(x_t)$が小さくなる。→$\alpha$が小さくなる。

# THE NEVER-GIVEUP AGENT

前節で示した内部報酬はエージェントに探索を推奨する要素である。訓練の際は探索-活用をスムーズに切り替える機構が必要になるため、ここでアーキテクチャに工夫を加えた。

**提案アーキテクチャ**:
universeal value function approximator(UVFA)を採用・本手法に適用した。UVFAでは複数の行動価値関数$Q(x, a, \beta_i)$を同時に近似する。N個のUVFAに対して離散値$\{\beta_i\}^{N-1}_{i=0}$を定義する。各$\beta_i$は0から最大値$\beta$の値をとる。仮に、探索せずに貪欲な動きをさせたい場合、$Q(x,a,0)$が貪欲なエージェントの価値関数となる。実用する上では、状況に合わせて$\beta$の値を切り替えることが可能になった。

モデルの詳細を次の図に示す。

!['model'](https://github.com/talesofyousan/ML_Paper/blob/master/ML_paper/never_give_up/img/model.png?raw=true)

# EXPERIMENTS

## CONTROLLED SETTING ANALYSIS
NGUでは観測した状態のうち、制御可能な状態だけを抽出して内部報酬の算出に役立てている。
この実験では、単純な環境を用意して制御可能な状態を抽出することで学習パフォーマンスの向上ぶりを示す。

**環境の説明 Ramdom Disco Maze**:

!['maze'](https://github.com/talesofyousan/ML_Paper/blob/master/ML_paper/never_give_up/img/maze.png?raw=true)

毎エピソード21×21サイズでランダムに迷路を生成。
エージェントのアクション: {左、右、上、下}
* 迷路の床部分の色は、エージェントのアクションとは関係なく切り替わる。 → 制御可能な状態だけを見て学習することが重要な環境
* 壁にぶつかった場合はそこでエピソード終了 → 失敗しやすいので報酬は疎になりやすい。

次の図ではユニークな状態(迷路の座標が異なっていること)の割合を表している。オレンジの線は提案手法、青い線は提案手法内のembedding関数fをランダムな実装にしただけの結果。
この図から、NGUによってエピソード内で積極的に迷路を探索していることがわかる。

エージェントのパフォーマンスについて積極的な移動を行なっていたが、ランダムなembedding関数をもつエージェントは2つの状態を行き来するだけで、ゴールには結びつかなかった

## ATARI RESULTS

### 探索が難しいゲームに対する実験
NGUのエージェントを実際にATARI2600に適用した結果表1に示す。

!['table1'](https://github.com/talesofyousan/ML_Paper/blob/master/ML_paper/never_give_up/img/table1.png?raw=true)

これらのゲームはAtari2600内でも特に探索が困難であると言われたゲームである。提案手法は、すべてのハードな探索タスクにおいて、state-of-the-artと同等かそれ以上の平均スコアを達成している。
特筆すべき点として、NGUはPitfall!で初めて正のスコアを獲得できたRL手法である。

ゲーム内容によっては、エージェントの種類が1種類の設定の方がスコアがよかったり長期記憶のありの方が劣っていたりなどといったケースが見られた。
前者については、ゲームによっては探索的な振る舞いと活用的な振る舞いが似ている場合、このような現象が起こるといったことが考えられる。
後者については、データ効率が悪化するためであると述べているが具体的には不明

### 報酬が密に得られるゲームに対する実験
報酬が密に得られるゲームを選んでSOTAの手法と比較した。

!['table2'](https://github.com/talesofyousan/ML_Paper/blob/master/ML_paper/never_give_up/img/table2.png?raw=true)

NGU(N=1)のエージェントは人間のスコアは上回っているものの、多くのゲームでR2D2に勝つことはできなかった。これは内部報酬が外部報酬のノイズとなっていることが原因であると考えられる。
また、N=32のように複数エージェントで学習する条件は軒並みスコアが奮わなかった。原因としては、探索的方策が活用的方策に干渉しているのではないかと考えている。それぞれのエージェントに非共有な設定を追加すれば改善する可能性はある。

# 読んでみた感想
* エピソード内の新規性を推定するテクニックはなかなか面白く、応用が効きそうだなと思った。
* シンプルなエージェントでは同じ状態を繰り返し探索してしまうケースには特に有効な方法といえる。(案件でそのようなシチュエーションが来るのかはわからないが)
* Strehl&Littmanらの最適な内部報酬の式が気になる。